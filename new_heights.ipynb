{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tYGEd9mCjzUF"
      },
      "outputs": [],
      "source": [
        "pip install llama-index llama-hub youtube_transcript_api"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XlIuDOkGjzUK"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from llama_hub.youtube_transcript import YoutubeTranscriptReader\n",
        "from llama_index import PromptHelper, ServiceContext, VectorStoreIndex\n",
        "from llama_index.embeddings import HuggingFaceEmbedding\n",
        "from llama_index.llms import HuggingFaceLLM\n",
        "from llama_index.prompts.prompts import SimpleInputPrompt\n",
        "\n",
        "# Globals\n",
        "model_name = \"HuggingFace4/zephyr-7b-beta\"\n",
        "tokenizer = \"BAAI/bge-small-en-v1.5\"\n",
        "\n",
        "# Youtube loader\n",
        "new_heights_vids = [\"https://www.youtube.com/watch?v=Zhv7aMcaCKM\"]\n",
        "loader = YoutubeTranscriptReader()\n",
        "documents = loader.load_data(ytlinks=new_heights_vids)\n",
        "\n",
        "# Prompts\n",
        "system_prompt = (\n",
        "    \"<|SYSTEM|> You are a helpful, resourceful, and friendly \"\n",
        "    + \"chatbot who always responds will well thought out answers.\"\n",
        ")\n",
        "\n",
        "query_wrapper_prompt = SimpleInputPrompt(\"<|USER|>{query_str}<|ASSISTANT|>\")\n",
        "prompt_helper = PromptHelper(\n",
        "    max_input_size=4096,\n",
        "    num_output=256,\n",
        "    max_chunk_overlap=0.2,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NDOepS9jjzUL"
      },
      "outputs": [],
      "source": [
        "# Models\n",
        "embedding = HuggingFaceEmbedding(tokenizer, trust_remote_code=True)\n",
        "llm = HuggingFaceLLM(\n",
        "    system_prompt=system_prompt,\n",
        "    query_wrapper_prompt=\"\",\n",
        "    model_name=model_name,\n",
        "    tokenizer_name=tokenizer,\n",
        "    model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
        "    generate_kwargs={\n",
        "        \"do_sample\": True,\n",
        "        \"temperature\": 0.7,\n",
        "        \"top_k\": 10,\n",
        "        \"top_p\": 0.95,\n",
        "    },\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gtefjDEZjzUM"
      },
      "outputs": [],
      "source": [
        "# Vector Store\n",
        "service_context = ServiceContext.from_defaults(\n",
        "    chunk_size=1024, llm=llm, embed_model=embedding, prompt_helper=prompt_helper\n",
        ")\n",
        "index = VectorStoreIndex.from_documents(documents, service_context=service_context)\n",
        "\n",
        "# persist\n",
        "index.storage_context.persist(\"vector_db\")\n",
        "\n",
        "query_engine = index.as_query_engine()\n",
        "query_engine.query(\n",
        "    \"Summarize what Travis Kelce said about the Chiefs' loss to the Broncos.\"\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}